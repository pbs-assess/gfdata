---
title: "gfdata Vignette"
author: "Elise Keppel"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{gfdata Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  eval = TRUE,
  cache = TRUE,
  autodep = TRUE,
  fig.path = "vignettes/knitr-figs/",
  cache.path = "vignettes/knitr-cache/"
)
```

```{r, echo=FALSE, cache=FALSE}
.error <- tryCatch(get_ssids(), error = function(e) "Error")
.eval <- !identical(class(.error), "character")
```

# Setup

If you don't already have the package installed, then run:

```{r, eval=FALSE}
# install.packages("devtools")
devtools::install_github("pbs-assess/gfdata")
```
    
First we will load the package along with dplyr since we will use it within our code later.

```{r, cache=FALSE, warning = FALSE, message = FALSE}
library(gfdata)
library(dplyr)
```


# An overview of gfdata

Commercial and research catch, effort, and biological data for groundfish are
archived by the DFO Pacific Groundfish Data Unit (Fisheries and Oceans Canada, Science
Branch, Pacific Region) and housed in a number of relational databases archived 
on-site at the Pacific Biological Station, Nanaimo, BC). 

The gfdata package was 
develeoped to automate data extraction from these databases in a consistent, 
reproducible manner with a series of `get_*()` functions. The functions extract
data using SQL queries, developed with support from the Groundfish Data Unit.
The standardized datasets are designed to feed directly into functions in the
gfplot package, or can of course also be analyzed outside of gfplot.\
\
The SQL code called in the `get_*()` functions can be viewed here:\
<https://github.com/pbs-assess/gfata/tree/master/inst/sql>\
\
How the various functions fit together:\
<https://github.com/pbs-assess/gfplot/blob/master/inst/function-web.pdf>\
\
Detailed information on the data extraction and the `get_*()` functions can be
found in:

Anderson, S.C., Keppel, E.A., Edwards, A.M.. "A reproducible data
synopsis for over 100 species of British Columbia groundfish". DFO Can. Sci.
Advis. Sec. REs. Doc. 2019/nnn. iv + 327 p.\
\
The complete list of `get_*()` functions in gfdata is:

```{r list-get}
fns <- ls("package:gfdata")
sort(fns[grepl("get", fns)])
```


The `get_*()` functions extract data by species, and some 
functions have arguments for additional filtering, such as survey series, 
management area, years, gear type, or environmental data type. In all cases, 
the `get_*()` functions can extract data for one or multiple species.

All functions can be viewed with the available arguments in the help
documentation for each set of functions with:

```{r, eval = FALSE}
?get_data
?get_environmental_data
?get_lookup_tables
```


In addition, a number of the `get` functions retain many relevant database 
columns that users can filter on with, for example, `dplyr::filter(dat, x = "y")`.


# Example

As an example, we could extract Pacific cod survey sample data with the
following function call if we were on a DFO laptop, with appropriate database 
permissions, and on the PBS network.

```{r, eval=.eval}
dat <- get_survey_samples("pacific cod")
head(dat)
```

Note that there are some duplicate records in the databases due to relating a 
record to multiple stratification schemes for alternative analyses. If this 
occurs, a warning is given.

> "Duplicate specimen IDs are present because of overlapping survey stratifications. If working with the data yourelf, filter them after selecting specific surveys. For example, `dat <- dat[!duplicated(dat$specimen_id), ]`. The tidying and plotting functions within gfplot will do this for you."

Either species name or species code can be given as an argument, and
species name, if used, is not case-sensitive. The following all do the same thing:

```{r, eval=FALSE}
get_survey_samples("pacific cod")
get_survey_samples("Pacific cod")
get_survey_samples("PaCiFiC cOD")
get_survey_samples("222")
get_survey_samples(222)
```

To extract multiple species at once, give a list as the species argument:

```{r, eval=FALSE}
get_survey_samples(c("pacific ocean perch", "pacific cod"))
get_survey_samples(c(396, 222))
get_survey_samples(c(222, "pacific cod"))
```

We can further restrict the data extraction to a single trawl survey series 
by including the ssid (survey series id) argument. For a list of
survey series id codes, run the lookup function `get_ssids()`.

```{r, eval=.eval}
ssids <- get_ssids()
head(ssids)
```

Select desired ssid and include as argument (i.e. the Queen Charlotte
Sound bottom trawl survey):

```{r, eval = .eval}
dat <- get_survey_samples(222, ssid = 1)
head(dat)
```

```{r, echo = FALSE, eval=.eval}
dat <- dat %>% filter(survey_series_id == 1)
```

```{r, eval=.eval}
glimpse(dat)
```

# Caching the data from the SQL servers

In addition to the individual `get_*()` functions, there is a function 
`cache_pbs_data()` that runs all the `get_*()` functions and caches the data 
in a folder that you specify. This is useful to be able to have the data
available for working on later when not on the PBS network, and it saves
running the SQL queries (though the data do get updated occassionally and the 
most up-to-date data should usually be extracted for analysis).

The helper function `cache_pbs_data()` will extract all of the data for the
given species into a series of `.rds` files into whatever folder you specify to
the `path` argument. I'll wrap it in a quick check just to make sure we don't 
download the data twice if we build this document again.

```{r, eval = FALSE, eval=.eval}
cache_pbs_data("pacific cod", path = "pcod-cache")
```

And to call the list of output files:

```{r, eval=.eval}
dat  <- readRDS(file.path("pcod-cache", "pacific-cod.rds"))
head(dat)
```

And to call one object/dataframe (i.e. our survey sample data) from the list:

```{r, eval=.eval}
dat <- dat$survey_samples
head(dat)
```
